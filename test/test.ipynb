{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b'}\n",
      "[(1, 'a'), (2, 'b')]\n",
      "1 a\n",
      "2 b\n"
     ]
    }
   ],
   "source": [
    "dict1 = {1:\"a\", 2:\"b\"}\n",
    "print(dict1)\n",
    "s = list(dict1.items())\n",
    "print(s)\n",
    "for i in s:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'b'}\n"
     ]
    }
   ],
   "source": [
    "dict1[3]= \"b\"\n",
    "print(dict1)\n",
    "dict2 = {4:\"d\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'b', 4: 'd'}\n"
     ]
    }
   ],
   "source": [
    "dict1.update(dict2)\n",
    "print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw_ingestion_status inhance_injection source_system  status    table_name  \\\n",
      "0            Processed         Completed          alfa  Active  SampleTable1   \n",
      "1            Processed        inprogress          alfa  Active  SampleTable2   \n",
      "2            Processed              Fail          alfa  Active  SampleTable3   \n",
      "3              Pending              Fail          alfa  Active  SampleTable4   \n",
      "\n",
      "  file_name_time  \n",
      "0    file1_time1  \n",
      "1    file2_time2  \n",
      "2    file3_time3  \n",
      "3    file3_time4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [{'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'Completed'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable1'}, 'file_name_time': {'S': 'file1_time1'}}, {'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'inprogress'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable2'}, 'file_name_time': {'S': 'file2_time2'}}, {'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'Fail'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable3'}, 'file_name_time': {'S': 'file3_time3'}}, {'raw_ingestion_status': {'S': 'Pending'}, 'inhance_injection': {'S': 'Fail'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable4'}, 'file_name_time': {'S': 'file3_time4'}}]\n",
    "\n",
    "formatted_data = []\n",
    "\n",
    "for entry in data:\n",
    "    formatted_entry = {}\n",
    "    for key, value in entry.items():\n",
    "        formatted_entry[key] = value['S']\n",
    "    formatted_data.append(formatted_entry)\n",
    "\n",
    "df = pd.DataFrame(formatted_data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critical_process\n",
      "non_critical_process\n"
     ]
    }
   ],
   "source": [
    "list_table = [\"critical_process\", \"non_critical_process\"]\n",
    "for table_name in list_table:\n",
    "    print(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  raw_ingestion_status inhance_injection source_system  status    table_name  \\\n",
      "0            Processed         Completed          alfa  Active  SampleTable1   \n",
      "1            Processed        inprogress          alfa  Active  SampleTable2   \n",
      "2            Processed              Fail          alfa  Active  SampleTable3   \n",
      "3              Pending              Fail          alfa  Active  SampleTable4   \n",
      "\n",
      "  file_name_time  \n",
      "0    file1_time1  \n",
      "1    file2_time2  \n",
      "2    file3_time3  \n",
      "3    file3_time4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [{'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'Completed'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable1'}, 'file_name_time': {'S': 'file1_time1'}}, {'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'inprogress'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable2'}, 'file_name_time': {'S': 'file2_time2'}}, {'raw_ingestion_status': {'S': 'Processed'}, 'inhance_injection': {'S': 'Fail'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable3'}, 'file_name_time': {'S': 'file3_time3'}}, {'raw_ingestion_status': {'S': 'Pending'}, 'inhance_injection': {'S': 'Fail'}, 'source_system': {'S': 'alfa'}, 'status': {'S': 'Active'}, 'table_name': {'S': 'SampleTable4'}, 'file_name_time': {'S': 'file3_time4'}}]\n",
    "\n",
    "# Extracting data into a list of dictionaries\n",
    "formatted_data = [{key: value['S'] for key, value in entry.items()} for entry in data]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(formatted_data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/24 12:39:21 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/03/24 12:39:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/24 12:39:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred (ResourceNotFoundException) when calling the Scan operation: Requested resource not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m table_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcritical_process\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_critical_process\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m table_names:\n\u001b[0;32m---> 41\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdynamodb_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatted_df_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     df\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mDynamoDBHelper.formatted_df_spark\u001b[0;34m(self, table_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformatted_df_spark\u001b[39m(\u001b[38;5;28mself\u001b[39m, table_name):\n\u001b[1;32m     27\u001b[0m     output_res_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entries(table_name)\n\u001b[0;32m---> 28\u001b[0m     formatted_data \u001b[38;5;241m=\u001b[39m [{key: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m output_res_items]\n\u001b[1;32m     29\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mcreateDataFrame(formatted_data)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class DynamoDBHelper:\n",
    "    def __init__(self, region_name='us-east-1'):\n",
    "        self.dynamodb = boto3.client('dynamodb', region_name=region_name)\n",
    "        self.spark = self.get_spark_session()\n",
    "\n",
    "    def get_spark_session(self):\n",
    "        return SparkSession.builder \\\n",
    "            .appName(\"YourAppName\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_entries(self, table_name):\n",
    "        try:\n",
    "            response = self.dynamodb.scan(\n",
    "                TableName=table_name\n",
    "            )\n",
    "            items = response['Items']\n",
    "            return items\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            return None\n",
    "\n",
    "    def formatted_df_spark(self, table_name):\n",
    "        output_res_items = self.get_entries(table_name)\n",
    "        formatted_data = [{key: value['S'] for key, value in entry.items()} for entry in output_res_items]\n",
    "        df = self.spark.createDataFrame(formatted_data)\n",
    "        return df\n",
    "\n",
    "    # Other methods can be added here as needed\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    dynamodb_helper = DynamoDBHelper()\n",
    "    \n",
    "    table_names = [\"critical_process\", \"non_critical_process\"]\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        df = dynamodb_helper.formatted_df_spark(table_name)\n",
    "        print(f\"DataFrame for {table_name}:\")\n",
    "        df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
